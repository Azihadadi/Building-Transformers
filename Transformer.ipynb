{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Building Transformers"
      ],
      "metadata": {
        "id": "o9aSYTRkWrQe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install tensorflow pyarrow\n",
        "%pip install pandas\n",
        "%pip install scikit-learn\n",
        "%pip install matplotlib\n",
        "%pip install requests"
      ],
      "metadata": {
        "id": "ui8LRPgSW1x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import requests\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.layers import Layer, Dense, LayerNormalization, Dropout\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "wub34Ra2W2mT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generating synthetic stock price data**"
      ],
      "metadata": {
        "id": "Y_N8pjbXXBTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a synthetic stock price dataset\n",
        "np.random.seed(42)\n",
        "data_length = 2000  # Adjust data length as needed\n",
        "trend = np.linspace(100, 200, data_length)\n",
        "noise = np.random.normal(0, 2, data_length)\n",
        "synthetic_data = trend + noise\n",
        "\n",
        "# Create a DataFrame and save as 'stock_prices.csv'\n",
        "data = pd.DataFrame(synthetic_data, columns=['Close'])\n",
        "data.to_csv('stock_prices.csv', index=False)\n",
        "print(\"Synthetic stock_prices.csv created and loaded.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VfGZ3u1-W6g2",
        "outputId": "12245eea-31c9-4fdd-f286-47a1072edef8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synthetic stock_prices.csv created and loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "data = pd.read_csv('stock_prices.csv')\n",
        "data = data[['Close']].values\n",
        "\n",
        "# Normalize the data\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "data = scaler.fit_transform(data)\n",
        "\n",
        "# Prepare the data for training\n",
        "def create_dataset(data, time_step=1):\n",
        "    X, Y = [], []\n",
        "\n",
        "    for i in range(len(data)-time_step-1):\n",
        "        a = data[i:(i+time_step), 0]\n",
        "        X.append(a)\n",
        "        Y.append(data[i + time_step, 0])\n",
        "    return np.array(X), np.array(Y)\n",
        "\n",
        "time_step = 100\n",
        "X, Y = create_dataset(data, time_step)\n",
        "X = X.reshape(X.shape[0], X.shape[1], 1)\n",
        "\n",
        "print(\"Shape of X:\", X.shape)\n",
        "print(\"Shape of Y:\", Y.shape)"
      ],
      "metadata": {
        "id": "RYekbXbFXN08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Multi-Head Self-Attention Implementation**"
      ],
      "metadata": {
        "id": "_w7lFuQZXW3C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadSelfAttention(Layer):\n",
        "\n",
        "    def __init__(self, embed_dim, num_heads=8):\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.projection_dim = embed_dim // num_heads\n",
        "        self.query_dense = Dense(embed_dim)\n",
        "        self.key_dense = Dense(embed_dim)\n",
        "        self.value_dense = Dense(embed_dim)\n",
        "        self.combine_heads = Dense(embed_dim)\n",
        "\n",
        "\n",
        "    def attention(self, query, key, value):\n",
        "        score = tf.matmul(query, key, transpose_b=True)\n",
        "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
        "        scaled_score = score / tf.math.sqrt(dim_key)\n",
        "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
        "        output = tf.matmul(weights, value)\n",
        "        return output, weights\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, inputs):\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        query = self.query_dense(inputs)\n",
        "        key = self.key_dense(inputs)\n",
        "        value = self.value_dense(inputs)\n",
        "        query = self.split_heads(query, batch_size)\n",
        "        key = self.split_heads(key, batch_size)\n",
        "        value = self.split_heads(value, batch_size)\n",
        "        attention, _ = self.attention(query, key, value)\n",
        "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
        "        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim))\n",
        "        output = self.combine_heads(concat_attention)\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "itqOM-ZmXYM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Transformer block**"
      ],
      "metadata": {
        "id": "av5tFTRKaPHp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(Layer):\n",
        "\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            Dense(ff_dim, activation=\"relu\"),\n",
        "            Dense(embed_dim),\n",
        "        ])\n",
        "\n",
        "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = Dropout(rate)\n",
        "        self.dropout2 = Dropout(rate)\n",
        "\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)"
      ],
      "metadata": {
        "id": "Bwdi4XrBaQNV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Encoder Layer Implementation**"
      ],
      "metadata": {
        "id": "lSQg695vaVz5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(Layer):\n",
        "\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            Dense(ff_dim, activation=\"relu\"),\n",
        "            Dense(embed_dim),\n",
        "        ])\n",
        "\n",
        "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = Dropout(rate)\n",
        "        self.dropout2 = Dropout(rate)\n",
        "\n",
        "\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n"
      ],
      "metadata": {
        "id": "3hZAAR8Qaaa3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Transformer Encoder Implementation**"
      ],
      "metadata": {
        "id": "_DSzHOF_agr0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(Layer):\n",
        "    def __init__(self, num_layers, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.embed_dim = embed_dim\n",
        "        self.enc_layers = [TransformerBlock(embed_dim, num_heads, ff_dim, rate) for _ in range(num_layers)]\n",
        "        self.dropout = Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        x = inputs\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x, training=training)\n",
        "        return x\n",
        "\n",
        "# Example usage\n",
        "embed_dim = 128\n",
        "num_heads = 8\n",
        "ff_dim = 512\n",
        "num_layers = 4\n",
        "\n",
        "transformer_encoder = TransformerEncoder(num_layers, embed_dim, num_heads, ff_dim)\n",
        "inputs = tf.random.uniform((1, 100, embed_dim))\n",
        "outputs = transformer_encoder(inputs, training=False)  # Use keyword argument for 'training'\n",
        "print(outputs.shape)  # Should print (1, 100, 128)"
      ],
      "metadata": {
        "id": "nODEimlbajr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Build and Compile the Transformer model**"
      ],
      "metadata": {
        "id": "ot7MrY0ZbdsS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = 128\n",
        "num_heads = 8\n",
        "ff_dim = 512\n",
        "num_layers = 4\n",
        "\n",
        "# Define the Transformer Encoder\n",
        "transformer_encoder = TransformerEncoder(num_layers, embed_dim, num_heads, ff_dim)\n",
        "\n",
        "# Build the model\n",
        "input_shape = (X.shape[1], X.shape[2])\n",
        "inputs = tf.keras.Input(shape=input_shape)\n",
        "\n",
        "# Project the inputs to the embed_dim\n",
        "x = tf.keras.layers.Dense(embed_dim)(inputs)\n",
        "encoder_outputs = transformer_encoder(x)\n",
        "flatten = tf.keras.layers.Flatten()(encoder_outputs)\n",
        "outputs = tf.keras.layers.Dense(1)(flatten)\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Summary of the model\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "bjfgKJjZbfNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train the Transformer model**"
      ],
      "metadata": {
        "id": "YXuLUi1ZbtKV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "model.fit(X, Y, epochs=20, batch_size=32)"
      ],
      "metadata": {
        "id": "TbAAX9TkbuFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluate and Make Predictions**"
      ],
      "metadata": {
        "id": "Ujq_pWdubx1m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions\n",
        "predictions = model.predict(X)\n",
        "predictions = scaler.inverse_transform(predictions)\n",
        "\n",
        "\n",
        "# Plot the predictions\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(data, label='True Data')\n",
        "plt.plot(np.arange(time_step, time_step + len(predictions)), predictions, label='Predictions')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Stock Price')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5NhvEPyZby8m"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}